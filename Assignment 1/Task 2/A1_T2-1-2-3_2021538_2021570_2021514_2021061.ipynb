{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 22:51:30.242747: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-05 22:51:30.299244: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-05 22:51:30.573934: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-05 22:51:30.574005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-05 22:51:30.630059: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-05 22:51:30.749306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-05 22:51:31.775040: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/akash/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True,)\n",
    "\n",
    "def emotion_scores(sample): \n",
    "    emotion=classifier(sample)\n",
    "    return emotion[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5429/5429 [01:31<00:00, 59.62it/s]\n",
      "100%|██████████| 5429/5429 [00:19<00:00, 274.40it/s]\n",
      "100%|██████████| 5429/5429 [00:20<00:00, 262.63it/s]\n",
      "100%|██████████| 5429/5429 [00:23<00:00, 234.64it/s]\n",
      "100%|██████████| 5429/5429 [00:23<00:00, 230.70it/s]\n",
      "100%|██████████| 5429/5429 [00:24<00:00, 219.36it/s]\n",
      "100%|██████████| 5429/5429 [00:23<00:00, 233.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[9.01219461e-05, 4.77863917e-05, 1.10238787e-04, ...,\n",
       "         1.06249795e-04, 7.52790104e-05, 9.02314998e-05],\n",
       "        [8.95601668e-05, 4.66632050e-05, 3.23000770e-04, ...,\n",
       "         1.06249497e-04, 7.52787991e-05, 9.02312465e-05],\n",
       "        [4.34680083e-04, 4.66739407e-05, 1.10263839e-04, ...,\n",
       "         1.06273941e-04, 7.52961182e-05, 9.02520056e-05],\n",
       "        ...,\n",
       "        [8.95601668e-05, 4.66632050e-05, 1.10238477e-04, ...,\n",
       "         1.06249497e-04, 7.52787991e-05, 9.02312465e-05],\n",
       "        [8.95601668e-05, 4.66632050e-05, 1.10238477e-04, ...,\n",
       "         1.06249497e-04, 7.52787991e-05, 9.02312465e-05],\n",
       "        [8.95601668e-05, 4.66632050e-05, 1.10238477e-04, ...,\n",
       "         1.06249497e-04, 7.52787991e-05, 9.02312465e-05]],\n",
       "\n",
       "       [[4.43063252e-05, 5.63282125e-05, 1.57215494e-04, ...,\n",
       "         3.30189472e-05, 4.35571107e-05, 9.46374619e-06],\n",
       "        [4.41312810e-05, 5.59781523e-05, 2.23524916e-04, ...,\n",
       "         3.30189183e-05, 4.35570726e-05, 9.46373791e-06],\n",
       "        [1.51671170e-04, 5.59821654e-05, 1.57226627e-04, ...,\n",
       "         3.30212855e-05, 4.35601952e-05, 9.46441638e-06],\n",
       "        ...,\n",
       "        [4.41312810e-05, 5.59781523e-05, 1.57215356e-04, ...,\n",
       "         3.30189183e-05, 4.35570726e-05, 9.46373791e-06],\n",
       "        [4.41312810e-05, 5.59781523e-05, 1.57215356e-04, ...,\n",
       "         3.30189183e-05, 4.35570726e-05, 9.46373791e-06],\n",
       "        [4.41312810e-05, 5.59781523e-05, 1.57215356e-04, ...,\n",
       "         3.30189183e-05, 4.35570726e-05, 9.46373791e-06]],\n",
       "\n",
       "       [[3.19863979e-05, 5.18039829e-05, 9.88592034e-05, ...,\n",
       "         2.87723361e-05, 6.29307301e-05, 1.08522979e-05],\n",
       "        [2.98035683e-05, 4.74384565e-05, 9.25799999e-04, ...,\n",
       "         2.87720221e-05, 6.29300434e-05, 1.08521795e-05],\n",
       "        [1.37201696e-03, 4.74809041e-05, 9.89465820e-05, ...,\n",
       "         2.87977671e-05, 6.29863527e-05, 1.08618899e-05],\n",
       "        ...,\n",
       "        [2.98035683e-05, 4.74384565e-05, 9.88581246e-05, ...,\n",
       "         2.87720221e-05, 6.29300434e-05, 1.08521795e-05],\n",
       "        [2.98035683e-05, 4.74384565e-05, 9.88581246e-05, ...,\n",
       "         2.87720221e-05, 6.29300434e-05, 1.08521795e-05],\n",
       "        [2.98035683e-05, 4.74384565e-05, 9.88581246e-05, ...,\n",
       "         2.87720221e-05, 6.29300434e-05, 1.08521795e-05]],\n",
       "\n",
       "       [[3.63070416e-04, 3.76986966e-04, 2.20538657e-04, ...,\n",
       "         3.68346971e-04, 3.55701903e-04, 2.21493792e-04],\n",
       "        [3.62953063e-04, 3.76752465e-04, 2.64922816e-04, ...,\n",
       "         3.68346755e-04, 3.55701695e-04, 2.21493663e-04],\n",
       "        [4.34948455e-04, 3.76770544e-04, 2.20549110e-04, ...,\n",
       "         3.68364430e-04, 3.55718763e-04, 2.21504291e-04],\n",
       "        ...,\n",
       "        [3.62953063e-04, 3.76752465e-04, 2.20538528e-04, ...,\n",
       "         3.68346755e-04, 3.55701695e-04, 2.21493663e-04],\n",
       "        [3.62953063e-04, 3.76752465e-04, 2.20538528e-04, ...,\n",
       "         3.68346755e-04, 3.55701695e-04, 2.21493663e-04],\n",
       "        [3.62953063e-04, 3.76752465e-04, 2.20538528e-04, ...,\n",
       "         3.68346755e-04, 3.55701695e-04, 2.21493663e-04]],\n",
       "\n",
       "       [[6.64534294e-05, 3.61933205e-05, 1.78135393e-04, ...,\n",
       "         6.30624318e-05, 8.40056160e-05, 4.30748643e-04],\n",
       "        [6.61921658e-05, 3.56709196e-05, 2.77095054e-04, ...,\n",
       "         6.30623495e-05, 8.40055063e-05, 4.30748081e-04],\n",
       "        [2.26691922e-04, 3.56747362e-05, 1.78154220e-04, ...,\n",
       "         6.30690968e-05, 8.40144945e-05, 4.30794169e-04],\n",
       "        ...,\n",
       "        [6.61921658e-05, 3.56709196e-05, 1.78135160e-04, ...,\n",
       "         6.30623495e-05, 8.40055063e-05, 4.30748081e-04],\n",
       "        [6.61921658e-05, 3.56709196e-05, 1.78135160e-04, ...,\n",
       "         6.30623495e-05, 8.40055063e-05, 4.30748081e-04],\n",
       "        [6.61921658e-05, 3.56709196e-05, 1.78135160e-04, ...,\n",
       "         6.30623495e-05, 8.40055063e-05, 4.30748081e-04]],\n",
       "\n",
       "       [[4.65777935e-05, 4.90419554e-05, 3.07182999e-04, ...,\n",
       "         4.36134855e-05, 6.92982292e-05, 1.06240933e-04],\n",
       "        [4.29329408e-05, 4.17530538e-05, 1.68789052e-03, ...,\n",
       "         4.36126908e-05, 6.92969665e-05, 1.06238997e-04],\n",
       "        [2.28533843e-03, 4.18154702e-05, 3.07636600e-04, ...,\n",
       "         4.36778872e-05, 6.94005582e-05, 1.06397813e-04],\n",
       "        ...,\n",
       "        [4.29329408e-05, 4.17530538e-05, 3.07177402e-04, ...,\n",
       "         4.36126908e-05, 6.92969665e-05, 1.06238997e-04],\n",
       "        [4.29329408e-05, 4.17530538e-05, 3.07177402e-04, ...,\n",
       "         4.36126908e-05, 6.92969665e-05, 1.06238997e-04],\n",
       "        [4.29329408e-05, 4.17530538e-05, 3.07177402e-04, ...,\n",
       "         4.36126908e-05, 6.92969665e-05, 1.06238997e-04]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramLM:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.word_idx = {}\n",
    "        for i in range(self.vocab_size):\n",
    "            self.word_idx[self.vocab[i]] = i\n",
    "        self.bigram_counts = np.zeros((self.vocab_size, self.vocab_size))\n",
    "        self.unigram_counts = np.zeros(self.vocab_size)\n",
    "        self.unigram_prob = np.zeros(self.vocab_size)\n",
    "        self.bigram_prob = np.zeros((self.vocab_size, self.vocab_size))\n",
    "        self.emotion_matrix = np.zeros((6,self.vocab_size,self.vocab_size))\n",
    "        self.unigram_emotions = np.zeros((6,self.vocab_size))\n",
    "        self.emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "        self.stop_words = []\n",
    "\n",
    "    def top_bigrams(self, probability_matrix):\n",
    "        # Flatten the matrix and sort by probability\n",
    "        flat_probs = probability_matrix.flatten()\n",
    "        top_indices = np.argsort(flat_probs)[::-1][:5]  # Get indices of top 5 probabilities\n",
    "        top_probs = flat_probs[top_indices]\n",
    "        \n",
    "        # Convert flat indices back to bigram indices\n",
    "        top_bigrams = [(self.vocab[i // self.vocab_size], self.vocab[i % self.vocab_size])\n",
    "                       for i in top_indices]\n",
    "        return top_bigrams, top_probs\n",
    "\n",
    "    def calc_emotion(self):\n",
    "        for i in tqdm(range(self.vocab_size)):\n",
    "            scores = emotion_scores(self.vocab[i])\n",
    "            for j in range(6):\n",
    "                self.unigram_emotions[j][i] = scores[j]['score']\n",
    "\n",
    "    def learn_from_dataset(self, dataset):\n",
    "        for sentence in dataset:\n",
    "            for word in sentence:\n",
    "                word_idx = self.word_idx[word]\n",
    "                self.unigram_counts[word_idx] += 1\n",
    "\n",
    "            for i in range(1, len(sentence)):\n",
    "                prev_word_idx = self.word_idx[sentence[i-1]]\n",
    "                curr_word_idx = self.word_idx[sentence[i]]\n",
    "                \n",
    "                self.bigram_counts[prev_word_idx, curr_word_idx] += 1\n",
    "\n",
    "        self.unigram_prob = self.unigram_counts/np.sum(self.unigram_counts)\n",
    "        self.calc_emotion()\n",
    "\n",
    "        #generating list of stop words\n",
    "        for i in dataset:\n",
    "            self.stop_words.append(i[-1])\n",
    "        freq_word = {}\n",
    "        for word in self.stop_words:\n",
    "            if word not in freq_word:\n",
    "                freq_word[word] = 1\n",
    "            else:\n",
    "                freq_word[word] += 1\n",
    "\n",
    "        \n",
    "        sorted_dict = sorted(freq_word.items(), key=lambda item: item[1], reverse=True)[:100]\n",
    "        top_words = [item[0] for item in sorted_dict]\n",
    "        self.stop_words = top_words\n",
    "\n",
    "        for i in range(len(self.bigram_counts)):\n",
    "            self.bigram_prob[i] = self.bigram_counts[i] / self.unigram_counts[i]\n",
    "        \n",
    "        for i in range(6):\n",
    "            for j in tqdm(range(self.vocab_size)):\n",
    "                for k in range(self.vocab_size):\n",
    "                    self.emotion_matrix[i][j][k] = self.bigram_prob[j][k] + self.unigram_emotions[i][k]\n",
    "                self.emotion_matrix[i][j] /= np.sum(self.emotion_matrix[i][j])\n",
    "\n",
    "        return self.emotion_matrix\n",
    "    \n",
    "    def laplace_smoothing(self,debug=False):\n",
    "        smoothed_probabilities = np.zeros((self.vocab_size, self.vocab_size))\n",
    "\n",
    "        for i in range(len(self.bigram_counts)):\n",
    "            smoothed_probabilities[i] = (self.bigram_counts[i] + 1) / (self.unigram_counts[i] + self.vocab_size)\n",
    "\n",
    "        if(debug):\n",
    "            print(smoothed_probabilities)\n",
    "        return smoothed_probabilities\n",
    "\n",
    "    def kneser_ney_smoothing(self, discount=0.75, debug=True):\n",
    "    \n",
    "        # Initialize the KN smoothed probability matrix\n",
    "        kneser_ney_prob = np.zeros((self.vocab_size, self.vocab_size))\n",
    "\n",
    "        # Calculate total number of bigrams\n",
    "        total_bigrams = np.sum(self.bigram_counts > 0)\n",
    "\n",
    "        # Calculate continuation probabilities\n",
    "        continuation_prob = np.zeros(self.vocab_size)\n",
    "        for word_idx in range(self.vocab_size):\n",
    "            continuation_prob[word_idx] = len(np.where(self.bigram_counts[:, word_idx] > 0)[0]) / total_bigrams\n",
    "\n",
    "        # Calculate lambda for each word\n",
    "        lambda_ = np.zeros(self.vocab_size)\n",
    "\n",
    "        for prev_word_idx in range(self.vocab_size):\n",
    "            # The number of word types that can follow w_(i-1)\n",
    "            continuation_types = len(np.where(self.bigram_counts[prev_word_idx, :] > 0)[0])\n",
    "            bigram_count_sum = np.sum(self.bigram_counts[prev_word_idx, :])\n",
    "            if bigram_count_sum > 0:\n",
    "                lambda_[prev_word_idx] = (discount * continuation_types) / bigram_count_sum\n",
    "            else:\n",
    "                lambda_[prev_word_idx] = 0  # Avoid division by zero if there are no following words\n",
    "\n",
    "        # Calculate adjusted probabilities for each bigram\n",
    "        for i in range(self.vocab_size):\n",
    "            for j in range(self.vocab_size):\n",
    "                if self.unigram_counts[i] > 0:\n",
    "                    # Apply the discount and divide by the unigram count\n",
    "                    kneser_ney_prob[i, j] = max(self.bigram_counts[i, j] - discount, 0) / self.unigram_counts[i]\n",
    "                    \n",
    "                    # Add the lambda times the continuation probability for the next word\n",
    "                    kneser_ney_prob[i, j] += lambda_[i] * continuation_prob[j]\n",
    "\n",
    "        # Update the class property with the new probabilities\n",
    "        self.bigram_prob = kneser_ney_prob\n",
    "\n",
    "        return kneser_ney_prob  \n",
    "    \n",
    "    \n",
    "    def generate_next_word(self, current_word, emotion,k = 4):\n",
    "        if current_word not in self.vocab:\n",
    "            raise ValueError(\"Word not in vocabulary\")\n",
    "        \n",
    "        if emotion not in self.emotions:\n",
    "            raise ValueError(\"Invalid emotion\")\n",
    "        \n",
    "        current_word_idx = self.word_idx[current_word]\n",
    "        emotion_idx = self.emotions.index(emotion)\n",
    "        next_word_probs = self.emotion_matrix[emotion_idx][current_word_idx]\n",
    "\n",
    "        sorted_indices = np.argsort(next_word_probs)\n",
    "        topk = sorted_indices[-k:]\n",
    "        next_word_idx = np.random.choice(topk)\n",
    "        return self.vocab[next_word_idx]\n",
    "    \n",
    "    def generate_sentence(self, initial_word, emotion, length = 6, k=4):\n",
    "        sentence = [initial_word]\n",
    "        while(length):\n",
    "            next = self.generate_next_word(sentence[-1],emotion=emotion,k=k)\n",
    "            sentence.append(next)\n",
    "            if next in self.stop_words and len(sentence)>2:\n",
    "                break\n",
    "            length-=1\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "file_path = 'corpus.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    dataset = []\n",
    "    for line in file:\n",
    "        dataset.append(line.lower().split())\n",
    "\n",
    "flat_dataset = [word for sentence in dataset for word in sentence]\n",
    "\n",
    "vocab = []\n",
    "\n",
    "for sentence in dataset:\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "\n",
    "bigram_model = BigramLM(vocab)\n",
    "bigram_model.learn_from_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9987954728191153\n",
      "0.9994575829898025\n"
     ]
    }
   ],
   "source": [
    "k = bigram_model.kneser_ney_smoothing(debug=True)\n",
    "print(np.sum(k[0]))\n",
    "l = bigram_model.laplace_smoothing(debug=False)\n",
    "print(np.sum(l[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "start_words = set()\n",
    "for i in dataset:\n",
    "    start_words.add(i[0])\n",
    "start_words = list(start_words)\n",
    "print(len(start_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i enthralled shocked'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.generate_sentence('i','surprise',length=10, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "file = open('gen.txt', 'w')\n",
    "label = open('gen_labels.txt', 'w')\n",
    "for emotion in emotions:\n",
    "    emotion_file = open(f\"gen_{emotion}.txt\", \"w\")\n",
    "    for i in range(50):\n",
    "        startword = random.choice(start_words)\n",
    "        x = bigram_model.generate_sentence(startword, emotion, 10, 108)\n",
    "        # print(emotion_scores(x))\n",
    "        file.write(x+'\\n')\n",
    "        emotion_file.write(x+'\\n')\n",
    "        label.write(emotion+'\\n')\n",
    "    emotion_file.close()\n",
    "label.close()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.34794585e-03,  3.30032321e-05,  5.72095524e-06, ...,\n",
       "        -1.04914516e-04, -1.04914516e-04, -1.04914516e-04],\n",
       "       [ 7.55063277e-03, -3.65337058e-05,  2.52215798e-02, ...,\n",
       "        -1.65441874e-04, -1.65441874e-04, -1.65441874e-04],\n",
       "       [ 1.49595757e-01, -1.18156411e-05,  5.01584856e-04, ...,\n",
       "        -1.61557453e-04, -1.61557453e-04, -1.61557453e-04],\n",
       "       ...,\n",
       "       [ 1.27066548e-02,  6.13773055e-05,  7.97995410e-04, ...,\n",
       "        -1.53469642e-04, -1.53469642e-04, -1.53469642e-04],\n",
       "       [ 1.27066548e-02,  6.13773055e-05,  7.97995410e-04, ...,\n",
       "        -1.53469642e-04, -1.53469642e-04, -1.53469642e-04],\n",
       "       [ 1.27066548e-02,  6.13773055e-05,  7.97995410e-04, ...,\n",
       "        -1.53469642e-04, -1.53469642e-04, -1.53469642e-04]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.bigram_prob-bigram_model.laplace_smoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 bigrams before smoothing:\n",
      "('kg', 'for'): 1.0\n",
      "('slopes', 'thats'): 1.0\n",
      "('gods', 'plan'): 1.0\n",
      "('dust', 'to'): 1.0\n",
      "('uw', 'school'): 1.0\n",
      "\n",
      "Top 5 bigrams after Laplace smoothing:\n",
      "('i', 'feel'): 0.11043610327619874\n",
      "('feel', 'like'): 0.0350976507217662\n",
      "('i', 'am'): 0.03189412019960946\n",
      "('that', 'i'): 0.02650602409638554\n",
      "('and', 'i'): 0.023103748910200523\n",
      "\n",
      "Top 5 bigrams after Kneser-Ney smoothing:\n",
      "('don', 't'): 0.9703597914718767\n",
      "('href', 'http'): 0.9700024553936815\n",
      "('didn', 't'): 0.9583674360233536\n",
      "('sort', 'of'): 0.956541337274802\n",
      "('supposed', 'to'): 0.9184706989687346\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Raw probabilities\n",
    "raw_probabilities = bigram_model.bigram_counts / bigram_model.unigram_counts[:, None]\n",
    "raw_top_bigrams, raw_top_probs = bigram_model.top_bigrams(raw_probabilities)\n",
    "\n",
    "# Laplace probabilities\n",
    "laplace_prob = bigram_model.laplace_smoothing()\n",
    "laplace_top_bigrams, laplace_top_probs = bigram_model.top_bigrams(laplace_prob)\n",
    "\n",
    "# Kneser-Ney probabilities\n",
    "kneser_ney_prob = bigram_model.kneser_ney_smoothing(discount=0.75)\n",
    "kneser_ney_top_bigrams, kneser_ney_top_probs = bigram_model.top_bigrams(kneser_ney_prob)\n",
    "\n",
    "# Print top 5 bigrams before smoothing\n",
    "print(\"Top 5 bigrams before smoothing:\")\n",
    "for bigram, prob in zip(raw_top_bigrams, raw_top_probs):\n",
    "    print(f\"{bigram}: {prob}\")\n",
    "\n",
    "# Print top 5 bigrams after Laplace smoothing\n",
    "print(\"\\nTop 5 bigrams after Laplace smoothing:\")\n",
    "for bigram, prob in zip(laplace_top_bigrams, laplace_top_probs):\n",
    "    print(f\"{bigram}: {prob}\")\n",
    "\n",
    "# Print top 5 bigrams after Kneser-Ney smoothing\n",
    "print(\"\\nTop 5 bigrams after Kneser-Ney smoothing:\")\n",
    "for bigram, prob in zip(kneser_ney_top_bigrams, kneser_ney_top_probs):\n",
    "    print(f\"{bigram}: {prob}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

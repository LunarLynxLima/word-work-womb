{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# def load_data(fp):\n",
    "#     with open(fp, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "#     return data\n",
    "\n",
    "# def save_data(data, fp):\n",
    "#     with open(fp, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# def tokenize(text):\n",
    "#     \"\"\"Custom tokenize method that returns tokens and their character span indices.\"\"\"\n",
    "#     tokens = []\n",
    "#     char_indices = []\n",
    "#     start_index = None\n",
    "#     for i, char in enumerate(text):\n",
    "#         if char.isalnum():\n",
    "#             if start_index is None:\n",
    "#                 start_index = i  # Mark the start of a new token\n",
    "#         else:\n",
    "#             if start_index is not None:\n",
    "#                 # End of the current token\n",
    "#                 tokens.append(text[start_index:i])\n",
    "#                 char_indices.append((start_index, i - 1))\n",
    "#                 start_index = None\n",
    "#     if start_index is not None:\n",
    "#         # Capture the last token if the string ends with an alphanumeric character\n",
    "#         tokens.append(text[start_index:])\n",
    "#         char_indices.append((start_index, len(text) - 1))\n",
    "#     return tokens, char_indices\n",
    "\n",
    "\n",
    "\n",
    "# def bio_encode_entities(data):\n",
    "#     bio_data = {}\n",
    "#     ctr = 1\n",
    "#     for item in data:\n",
    "#         case_id = str(ctr)\n",
    "#         text = item['data']['text']\n",
    "#         annotations = item['annotations'][0]['result']\n",
    "        \n",
    "#         tokens, char_indices = tokenize(text)\n",
    "#         labels = ['O'] * len(tokens)\n",
    "        \n",
    "#         for annotation in annotations:\n",
    "#             label = annotation['value']['labels'][0]\n",
    "#             start_char = annotation['value']['start']\n",
    "#             end_char = annotation['value']['end'] - 1  # Adjust to inclusive end index\n",
    "#             for i, (start, end) in enumerate(char_indices):\n",
    "#                 if end_char < start:\n",
    "#                     break  # Past the relevant span\n",
    "#                 if start_char > end:\n",
    "#                     continue  # Not yet reached the relevant span\n",
    "#                 if start <= start_char <= end:\n",
    "#                     labels[i] = 'B_' + label\n",
    "#                 if start_char <= start and end <= end_char:\n",
    "#                     labels[i] = 'I_' + label if labels[i] != 'B_' + label else labels[i]\n",
    "        \n",
    "#         bio_text = \" \".join(tokens)\n",
    "#         bio_data[case_id] = {'text': bio_text, 'labels': labels}\n",
    "#         ctr+=1\n",
    "#     return bio_data\n",
    "\n",
    "# # Load, process, and save data\n",
    "# train_data = load_data('DATASET_TASK1/task1_train.json')\n",
    "# train_split, val_split = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "# train_bio = bio_encode_entities(train_split)\n",
    "# val_bio = bio_encode_entities(val_split)\n",
    "# test_data = load_data('DATASET_TASK1/task1_test.json')\n",
    "# test_bio = bio_encode_entities(test_data)\n",
    "\n",
    "# save_data(train_bio, 'NER_train.json')\n",
    "# save_data(val_bio, 'NER_val.json')\n",
    "# save_data(test_bio, 'NER_test.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def save_data(processed_data, fp,ensure_ascii=False,indent=4):\n",
    "    with open(fp, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_data, f, ensure_ascii=ensure_ascii, indent=indent)\n",
    "\n",
    "def tokenize(text):\n",
    "    char_indices = []\n",
    "    start_index = -1\n",
    "    tokens = []\n",
    "    \n",
    "    for i, char in enumerate(text):\n",
    "        if char.isalnum():\n",
    "            if start_index == -1:\n",
    "                start_index = i\n",
    "        else:\n",
    "            if start_index != -1:\n",
    "                tokens.append(text[start_index:i]) # appending current token\n",
    "                char_indices.append((start_index, i - 1)) # appending req. pair\n",
    "                start_index = -1\n",
    "\n",
    "    if start_index!=-1:\n",
    "        tokens.append(text[start_index:i]) # adding current token \n",
    "        char_indices.append((start_index, len(text) - 1)) # appending req. pair\n",
    "\n",
    "    return tokens, char_indices\n",
    "\n",
    "\n",
    "def bio_encode_entities(data):\n",
    "    ctr = 1\n",
    "    bio_data = {}\n",
    "    for item in data:\n",
    "        text = item['data']['text']\n",
    "        annotations = item['annotations'][0]['result']\n",
    "        \n",
    "        tokens, char_indices = tokenize(text)\n",
    "        labels = ['O' for _ in range(len(tokens))]\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            start_char = annotation['value']['start']\n",
    "            end_char = annotation['value']['end'] - 1  # Adjust to inclusive end index\n",
    "            label = annotation['value']['labels'][0]\n",
    "            for i, (start, end) in enumerate(char_indices):\n",
    "                if end_char < start:\n",
    "                    break  # Past the relevant span\n",
    "                if start_char > end:\n",
    "                    continue  # Not yet reached the relevant span\n",
    "                if start <= start_char and start_char <= end:\n",
    "                    labels[i] = 'B_' + label\n",
    "                if start_char <= start and end <= end_char:\n",
    "                    if labels[i] != 'B_' + label:\n",
    "                        labels[i] = 'I_' + label\n",
    "                    else:\n",
    "                        labels[i] = labels[i]\n",
    "        \n",
    "        bio_text = \" \".join(tokens)\n",
    "        bio_data[str(ctr)] = {'text': bio_text, 'labels': labels}\n",
    "        ctr+=1\n",
    "    return bio_data \n",
    "\n",
    "# Load, process, and save data\n",
    "folder = 'DATASET_TASK1/'\n",
    "file1 = folder + 'task1_train.json'\n",
    "file2 = folder + 'task1_test.json'\n",
    "\n",
    "with open(file1, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "with open(file2, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "train_split, val_split = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "train_bio = bio_encode_entities(train_split)\n",
    "val_bio = bio_encode_entities(val_split)\n",
    "test_bio = bio_encode_entities(test_data)\n",
    "\n",
    "save_data(train_bio, 'NER_train.json')\n",
    "save_data(val_bio , 'NER_val.json')\n",
    "save_data(test_bio, 'NER_test.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
